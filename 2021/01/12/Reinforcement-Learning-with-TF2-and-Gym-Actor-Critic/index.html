<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qiuzhaopeng.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Recently I started to do exercise with a new RL method: Actor-Critic. The basic idea is quite similar to GAN network. Unlike Policy Gradient in which a single neural network is trained, Actor-Critic u">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning with TF2 and Gym: Actor-Critic">
<meta property="og:url" content="http://qiuzhaopeng.github.io/2021/01/12/Reinforcement-Learning-with-TF2-and-Gym-Actor-Critic/index.html">
<meta property="og:site_name" content="Zhaopeng&#39;s Homepage">
<meta property="og:description" content="Recently I started to do exercise with a new RL method: Actor-Critic. The basic idea is quite similar to GAN network. Unlike Policy Gradient in which a single neural network is trained, Actor-Critic u">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://dev-to-uploads.s3.amazonaws.com/i/qyn1s4ckmrhdb5p2tsoq.PNG">
<meta property="og:image" content="https://dev-to-uploads.s3.amazonaws.com/i/xgrowra6h096oaoeho52.PNG">
<meta property="og:image" content="https://dev-to-uploads.s3.amazonaws.com/i/7a2lsxp4osi3vur2kop3.PNG">
<meta property="og:image" content="https://dev-to-uploads.s3.amazonaws.com/i/h7kwg09gwsep9a6np10v.PNG">
<meta property="article:published_time" content="2021-01-12T09:29:44.000Z">
<meta property="article:modified_time" content="2021-01-12T09:35:39.889Z">
<meta property="article:author" content="Zhaopeng QIU">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://dev-to-uploads.s3.amazonaws.com/i/qyn1s4ckmrhdb5p2tsoq.PNG">

<link rel="canonical" href="http://qiuzhaopeng.github.io/2021/01/12/Reinforcement-Learning-with-TF2-and-Gym-Actor-Critic/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Reinforcement Learning with TF2 and Gym: Actor-Critic | Zhaopeng's Homepage</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhaopeng's Homepage</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">My tech notebook</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    

  <a href="https://github.com/QiuZhaopeng" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qiuzhaopeng.github.io/2021/01/12/Reinforcement-Learning-with-TF2-and-Gym-Actor-Critic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/qiu.png">
      <meta itemprop="name" content="Zhaopeng QIU">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhaopeng's Homepage">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reinforcement Learning with TF2 and Gym: Actor-Critic
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-01-12 10:29:44 / Modified: 10:35:39" itemprop="dateCreated datePublished" datetime="2021-01-12T10:29:44+01:00">2021-01-12</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Recently I started to do exercise with a new RL method: <strong>Actor-Critic</strong>. The basic idea is quite similar to <strong>GAN network</strong>. Unlike <strong>Policy Gradient</strong> in which a single neural network is trained, <strong>Actor-Critic</strong> uses two neural networks: an “actor” for performing actions and a “critic” for evaluating action of the actor. I refer to <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xjd7Jq9wPQY">this page</a> for details of this method. </p>
<h3 id="Model-presentation"><a href="#Model-presentation" class="headerlink" title="Model presentation"></a>Model presentation</h3><p>Given a state <code>s_t</code> at time <em>t</em>, the network calculates the action probabilities and chooses an action <code>a_t</code> to apply on env:<br><img src="https://dev-to-uploads.s3.amazonaws.com/i/qyn1s4ckmrhdb5p2tsoq.PNG" alt="Alt Text"></p>
<p>After the application of action <code>a_t</code>, then env updated accordingly, yields reward <code>r_t</code>, we observe at next time step t+1 a new state <code>s_t+1</code> which results in a new sampled action <code>a_t+1</code>:<br><img src="https://dev-to-uploads.s3.amazonaws.com/i/xgrowra6h096oaoeho52.PNG" alt="Alt Text"></p>
<h3 id="Steps-of-the-basic-Actor-Critic-method"><a href="#Steps-of-the-basic-Actor-Critic-method" class="headerlink" title="Steps of the basic Actor-Critic method"></a>Steps of the basic Actor-Critic method</h3><p>Unlike the <strong>Policy Gradient</strong> which needs to gather all data of an episode to calculate gradients, Actor-Critic method performs model update in every agent-env interaction loop :</p>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/7a2lsxp4osi3vur2kop3.PNG" alt="Alt Text"></p>
<h3 id="Implementation-with-CartPole-game"><a href="#Implementation-with-CartPole-game" class="headerlink" title="Implementation with CartPole game"></a>Implementation with CartPole game</h3><p>I followed <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2vJtbAha3To">this youtube video</a> for my first Actor-Critic exercise. It was initially based on pure <strong>Keras</strong> thus I have modified some of his code for using <strong>tensorflow</strong>. I also disabled eager mode because of a runtime error (I am using Tensorflow v2 on my laptop).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Activation, Dense, Input</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, alpha, beta, gamma=<span class="number">0.99</span>, n_actions=<span class="number">2</span>, \</span></span></span><br><span class="line"><span class="function"><span class="params">                layer1_size=<span class="number">1024</span>, layer2_size=<span class="number">512</span>, input_dims=<span class="number">4</span></span>):</span></span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.beta = beta</span><br><span class="line">        self.input_dims = input_dims</span><br><span class="line">        self.fc1_dims = layer1_size</span><br><span class="line">        self.fc2_dims = layer2_size</span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        </span><br><span class="line">        self.actor, self.critic, self.policy = self.build_network()</span><br><span class="line">        </span><br><span class="line">        self.action_space = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_actions)]</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_network</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">input</span> = Input(shape=(self.input_dims, ))</span><br><span class="line">        delta = Input(shape=[<span class="number">1</span>])</span><br><span class="line">        dense1 = Dense(self.fc1_dims, activation=<span class="string">&quot;relu&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">        dense2 = Dense(self.fc2_dims, activation=<span class="string">&quot;relu&quot;</span>)(dense1)</span><br><span class="line">        probs = Dense(self.n_actions, activation=<span class="string">&quot;softmax&quot;</span>)(dense2)</span><br><span class="line">        values = Dense(<span class="number">1</span>, activation=<span class="string">&quot;linear&quot;</span>)(dense2)</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">custom_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">            out = K.clip(y_pred, <span class="number">1e-8</span>, <span class="number">1</span>-<span class="number">1e-8</span>)  </span><br><span class="line">            log_lik = y_true * K.log(out)            </span><br><span class="line">            <span class="keyword">return</span> K.<span class="built_in">sum</span>(-log_lik*delta)</span><br><span class="line">            </span><br><span class="line">        actor = Model(inputs=[<span class="built_in">input</span>, delta], outputs=[probs])</span><br><span class="line">        actor.<span class="built_in">compile</span>(optimizer=Adam(lr=self.alpha), loss=custom_loss)</span><br><span class="line">        </span><br><span class="line">        critic = Model(inputs=[<span class="built_in">input</span>], outputs=[values])</span><br><span class="line">        critic.<span class="built_in">compile</span>(optimizer=Adam(lr=self.beta), loss=<span class="string">&quot;mse&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        policy = Model(inputs=[<span class="built_in">input</span>], outputs=[probs])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> actor, critic, policy</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">    </span><br><span class="line">        state = observation[np.newaxis, :]</span><br><span class="line">        probs = self.policy.predict(state)[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        action = np.random.choice(self.action_space, p=probs)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, state, action, reward, state_, done</span>):</span></span><br><span class="line">        state = state[np.newaxis, :]</span><br><span class="line">        state_ = state_[np.newaxis, :]</span><br><span class="line">        critic_value_ = self.critic.predict(state_)</span><br><span class="line">        critic_value = self.critic.predict(state)</span><br><span class="line">        </span><br><span class="line">        target = reward + self.gamma*critic_value_*(<span class="number">1</span>-<span class="built_in">int</span>(done))</span><br><span class="line">        delta = target - critic_value</span><br><span class="line">        </span><br><span class="line">        actions = np.zeros([<span class="number">1</span>, self.n_actions])</span><br><span class="line">        actions[np.arange(<span class="number">1</span>), action] = <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        self.actor.fit([state, delta], actions, verbose=<span class="number">0</span>)</span><br><span class="line">        self.critic.fit(state, target, verbose=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    agent=Agent(alpha=<span class="number">0.0001</span>, beta=<span class="number">0.0005</span>)</span><br><span class="line">    env = gym.make(<span class="string">&quot;CartPole-v0&quot;</span>)</span><br><span class="line">    score_history = []</span><br><span class="line">    num_episodes = <span class="number">2000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            env.render()</span><br><span class="line">            action = agent.choose_action(observation)</span><br><span class="line">            observation_, reward, done, info= env.step(action)</span><br><span class="line">            agent.learn(observation, action, reward, observation_, done)</span><br><span class="line">            observation = observation_</span><br><span class="line">            score+=reward</span><br><span class="line">            </span><br><span class="line">        score_history.append(score)</span><br><span class="line">        avg_score = np.mean(score_history[-<span class="number">100</span>:])</span><br><span class="line">        print(<span class="string">&quot;episode &quot;</span>, i, <span class="string">&quot;score %.2f average score %.2f&quot;</span> % (score, avg_score))</span><br></pre></td></tr></table></figure>

<h3 id="A-newer-version"><a href="#A-newer-version" class="headerlink" title="A newer version"></a>A newer version</h3><p>I have updated the program shown above to allow eager mode. The main difference is the introduction of the custom loss function for actor. It works well, however, the running speed is much slower after my modification (I am looking forward to Refactoring it later).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&quot;2&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> tf.__version__.startswith(<span class="string">&quot;1.&quot;</span>):    </span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Error!! You are using tensorflow-v1&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Activation, Dense, Input</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, alpha, beta, gamma=<span class="number">0.99</span>, n_actions=<span class="number">2</span>, \</span></span></span><br><span class="line"><span class="function"><span class="params">                layer1_size=<span class="number">1024</span>, layer2_size=<span class="number">512</span>, input_dims=<span class="number">4</span></span>):</span></span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.beta = beta</span><br><span class="line">        self.input_dims = input_dims</span><br><span class="line">        self.fc1_dims = layer1_size</span><br><span class="line">        self.fc2_dims = layer2_size</span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        </span><br><span class="line">        self.actor, self.critic, self.policy = self.build_network()</span><br><span class="line">        </span><br><span class="line">        self.action_space = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_actions)]</span><br><span class="line"></span><br><span class="line">        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_network</span>(<span class="params">self</span>):</span></span><br><span class="line">    </span><br><span class="line">        <span class="built_in">input</span> = Input(shape=(self.input_dims, ))</span><br><span class="line">        delta = Input(shape=[<span class="number">1</span>])</span><br><span class="line">        dense1 = Dense(self.fc1_dims, activation=<span class="string">&quot;relu&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">        dense2 = Dense(self.fc2_dims, activation=<span class="string">&quot;relu&quot;</span>)(dense1)</span><br><span class="line">        probs = Dense(self.n_actions, activation=<span class="string">&quot;softmax&quot;</span>)(dense2)</span><br><span class="line">        values = Dense(<span class="number">1</span>, activation=<span class="string">&quot;linear&quot;</span>)(dense2)</span><br><span class="line"></span><br><span class="line">        actor = Model(inputs=[<span class="built_in">input</span>, delta], outputs=[probs])</span><br><span class="line">        </span><br><span class="line">        critic = Model(inputs=[<span class="built_in">input</span>], outputs=[values])</span><br><span class="line">        critic.<span class="built_in">compile</span>(optimizer=Adam(lr=self.beta), loss=<span class="string">&quot;mse&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        policy = Model(inputs=[<span class="built_in">input</span>], outputs=[probs])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> actor, critic, policy</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">    </span><br><span class="line">        state = observation[np.newaxis, :]</span><br><span class="line">        probs = self.policy.predict(state)[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        action = np.random.choice(self.action_space, p=probs)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, state, action, reward, state_, done</span>):</span></span><br><span class="line">        state = state[np.newaxis, :]</span><br><span class="line">        state_ = state_[np.newaxis, :]</span><br><span class="line">        critic_value_ = self.critic.predict(state_)</span><br><span class="line">        critic_value = self.critic.predict(state)</span><br><span class="line">        </span><br><span class="line">        target = reward + self.gamma*critic_value_*(<span class="number">1</span>-<span class="built_in">int</span>(done))</span><br><span class="line">        delta = target - critic_value</span><br><span class="line">        </span><br><span class="line">        actions = np.zeros([<span class="number">1</span>, self.n_actions])</span><br><span class="line">        actions[np.arange(<span class="number">1</span>), action] = <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        self.critic.fit(state, target, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y_pred = self.actor(state)</span><br><span class="line">            out = K.clip(y_pred, <span class="number">1e-8</span>, <span class="number">1</span>-<span class="number">1e-8</span>)  </span><br><span class="line">            log_lik = actions * K.log(out)            </span><br><span class="line">            myloss = K.<span class="built_in">sum</span>(-log_lik*delta)</span><br><span class="line">        grads = tape.gradient(myloss, self.actor.trainable_variables)</span><br><span class="line"></span><br><span class="line">        self.optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.actor.trainable_variables))</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    agent=Agent(alpha=<span class="number">0.0001</span>, beta=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line">    ENV_SEED = <span class="number">1024</span>  <span class="comment">## Reproducibility of the game</span></span><br><span class="line">    NP_SEED = <span class="number">1024</span>  <span class="comment">## Reproducibility of numpy random</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">    env = env.unwrapped    <span class="comment"># use unwrapped version, otherwise episodes will terminate after 200 steps</span></span><br><span class="line">    env.seed(ENV_SEED)  </span><br><span class="line">    np.random.seed(NP_SEED)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">### The Discrete space allows a fixed range of non-negative numbers, so in this case valid actions are either 0 or 1. </span></span><br><span class="line">    print(env.action_space)</span><br><span class="line">    <span class="comment">### The Box space represents an n-dimensional box, so valid observations will be an array of 4 numbers. </span></span><br><span class="line">    print(env.observation_space)</span><br><span class="line">    <span class="comment">### We can also check the Box’s bounds:</span></span><br><span class="line">    print(env.observation_space.high)</span><br><span class="line">    print(env.observation_space.low)</span><br><span class="line"></span><br><span class="line">    score_history = []</span><br><span class="line">    num_episodes = <span class="number">2000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            env.render()</span><br><span class="line">            action = agent.choose_action(observation)</span><br><span class="line">            observation_, reward, done, info= env.step(action)</span><br><span class="line">            agent.learn(observation, action, reward, observation_, done)</span><br><span class="line">            observation = observation_</span><br><span class="line">            score+=reward</span><br><span class="line">            </span><br><span class="line">        score_history.append(score)</span><br><span class="line">        avg_score = np.mean(score_history[-<span class="number">100</span>:])</span><br><span class="line">        print(<span class="string">&quot;episode &quot;</span>, i, <span class="string">&quot;score %.2f average score %.2f&quot;</span> % (score, avg_score))</span><br></pre></td></tr></table></figure>
<p>The training converges ideally. Below is a snapshot of the execution output:</p>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/h7kwg09gwsep9a6np10v.PNG" alt="Alt Text"></p>
<h5 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h5><p>*<a target="_blank" rel="noopener" href="https://github.com/wangshusen/DRL">https://github.com/wangshusen/DRL</a><br>*<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2vJtbAha3To">https://www.youtube.com/watch?v=2vJtbAha3To</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/01/11/A-A-Star-algorithm-for-solving-Sliding-Tiles-Game/" rel="prev" title="A* (A-Star) algorithm for solving Sliding Tiles Game">
      <i class="fa fa-chevron-left"></i> A* (A-Star) algorithm for solving Sliding Tiles Game
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/01/12/Native-Mobile-Application-dev-using-HTML5/" rel="next" title="Native Mobile Application dev using HTML5">
      Native Mobile Application dev using HTML5 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-presentation"><span class="nav-number">1.</span> <span class="nav-text">Model presentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Steps-of-the-basic-Actor-Critic-method"><span class="nav-number">2.</span> <span class="nav-text">Steps of the basic Actor-Critic method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementation-with-CartPole-game"><span class="nav-number">3.</span> <span class="nav-text">Implementation with CartPole game</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-newer-version"><span class="nav-number">4.</span> <span class="nav-text">A newer version</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#References"><span class="nav-number">4.0.1.</span> <span class="nav-text">References:</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhaopeng QIU"
      src="/images/qiu.png">
  <p class="site-author-name" itemprop="name">Zhaopeng QIU</p>
  <div class="site-description" itemprop="description">Stay hungry, stay foolish</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhaopeng QIU</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  















  

  

</body>
</html>
